{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from library.data import *\r\n",
    "from library.synonyms import *\r\n",
    "import nltk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "ld = LoadData()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading topics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "topicn,query = ld.load_topics()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading Corpus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "corpus = ld.load_corpus()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading query results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "qtopicn,idn,label = ld.load_query_results()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "for each query, we pass to the ir engine, which scans the corpus and returns query results. We then compare these query results against the query results data.\r\n",
    "\r\n",
    "because of the way the algorithm works, we need to collect synonyms up to num_synonyms \r\n",
    "\r\n",
    "We also need to normalize the resulting figure to take into account that some words will have many synonyms or big clusters, and some will have few. The scores for the ones with few synonyms, and long documents, will necessarily be lower, but that doesn't necessarily mean they are less relevant. Thus there ought to be a normalization factor which accounts for the number of synonyms/size of cluster and also the length of the document. (num unique tokens)\r\n",
    "\r\n",
    "We also need to have a maximum number of synonyms/clustered selected words as a parameter.\r\n",
    "\r\n",
    "add something to increase the number of synonyms, and add an associated discount factor for each synonym \"level\" you go out\r\n",
    "\r\n",
    "Do hyperparameter tuning via a grid search. Do a train test split as well, as you are effectively \"training\" the model by tuning the hyperparameters, and you don't want to overfit the hyperparameters to your results and then report those.\r\n",
    "\r\n",
    "Also do auc curve by varying decision boundary\r\n",
    "\r\n",
    "can you determine correct decision threshold by looking at the distribution of confidence values in the population? If you know that 5% of the documents will be relevant, perhaps ste the decision threshold to include 5% of documents? Or maybe 10%? If you know the precision and recall curves, then maybe you can create a function that takes in the auc curve, the percent estimated to be positivei in the population, and gives you predicted precision, recall, and accuracy, for each given decision boundary placement? This might be a proposal for discussion/future research section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "syn = Synonyms()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "syn.get('intelligence')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['acumen',\n",
       " 'agility',\n",
       " 'brilliance',\n",
       " 'intellect',\n",
       " 'judgment',\n",
       " 'perception',\n",
       " 'quickness',\n",
       " 'savvy',\n",
       " 'sense',\n",
       " 'skill',\n",
       " 'subtlety',\n",
       " 'understanding',\n",
       " 'wit',\n",
       " 'IQ',\n",
       " 'acuity',\n",
       " 'alertness',\n",
       " 'aptitude',\n",
       " 'brainpower',\n",
       " 'brains',\n",
       " 'brightness',\n",
       " 'capacity',\n",
       " 'cleverness',\n",
       " 'comprehension',\n",
       " 'coruscation',\n",
       " 'discernment',\n",
       " 'luminosity',\n",
       " 'mentality',\n",
       " 'mind',\n",
       " 'penetration',\n",
       " 'perspicacity',\n",
       " 'precocity',\n",
       " 'quotient',\n",
       " 'reason',\n",
       " 'sagacity',\n",
       " 'smarts',\n",
       " 'trenchancy']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "class SynonymIR:\r\n",
    "    def __init__(self,max_similar=20,similar_significance=0.5):\r\n",
    "        \"\"\" \r\n",
    "        \r\n",
    "        NOTE: I need to add the functionality to get synonyms of \r\n",
    "              synonyms in order to fill out the graph in contexts\r\n",
    "              where there are relatively few synonyms.\r\n",
    "        \"\"\"\r\n",
    "        self.__syn = Synonyms()\r\n",
    "        self.__ps = nltk.stem.PorterStemmer()\r\n",
    "        self.__max_similar = max_similar\r\n",
    "        self.__similar_significance = similar_significance\r\n",
    "\r\n",
    "    def preprocess_document(self,document):\r\n",
    "        document = document.replace('-',' ')\r\n",
    "        document = ''.join([i for i in document if (i.isalpha()) or (i == ' ')])\r\n",
    "        document = document.lower()\r\n",
    "        document = nltk.word_tokenize(document)\r\n",
    "        document = list(set(document))\r\n",
    "        # remove stopwords\r\n",
    "        document = [word for word in document if not word in nltk.corpus.stopwords.words()]\r\n",
    "        document = [self.__ps.stem(w) for w in document]\r\n",
    "        # stem\r\n",
    "        return set(document)\r\n",
    "\r\n",
    "    def preprocess_query(self,query):\r\n",
    "        # need to remove stopwords from query\r\n",
    "        query = [i for i in query.split() if i not in nltk.corpus.stopwords.words()]\r\n",
    "        # need to get synonyms\r\n",
    "        synonyms = []\r\n",
    "        for word in query:\r\n",
    "            synonyms.extend(self.__syn.get(word))\r\n",
    "    \r\n",
    "        # need to tokenize and stem synonyms and query\r\n",
    "        query = [self.__ps.stem(i) for i in query]\r\n",
    "        synonyms = list(set([self.__ps.stem(i) for i in synonyms]))\r\n",
    "        if len(synonyms) > self.__max_similar:\r\n",
    "            synonyms = synonyms[:self.__max_similar] # not a great selection mechanism, but not sure about alternative\r\n",
    "        # need to return each as a set\r\n",
    "        return set(query), set(synonyms)\r\n",
    "\r\n",
    "    def get_jaccard_score(self,A,B):\r\n",
    "        intersection = A.intersection(B)\r\n",
    "        symmetric_difference = A ^ B\r\n",
    "        return len(intersection)/(len(intersection)+len(symmetric_difference))\r\n",
    "    \r\n",
    "    def similarity_score(self,query_jaccard,synonyms_jaccard,similar_significance):\r\n",
    "        return (query_jaccard*(1-similar_significance))+(synonyms_jaccard*similar_significance)\r\n",
    "\r\n",
    "    def get_similarity(self,query,document):\r\n",
    "        query,synonyms = self.preprocess_query(query)\r\n",
    "        document = self.preprocess_document(document)\r\n",
    "        query_jaccard = self.get_jaccard_score(query,document)\r\n",
    "        synonyms_jaccard = self.get_jaccard_score(synonyms,document)\r\n",
    "        # we want the similar_significance to be between 0 and 1\r\n",
    "        # at 0, the query jaccard score should be 100% of the score\r\n",
    "        # at 1, the synonyms jaccard score should be 100% of the score\r\n",
    "        similarity_score = self.similarity_score(query_jaccard,synonyms_jaccard,self.__similar_significance)\r\n",
    "        return similarity_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "sir = SynonymIR()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "n = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "tq = query[0]\r\n",
    "td = corpus[0]['text']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "tq,ts = sir.preprocess_query(tq)\r\n",
    "td = sir.preprocess_document(td)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "td"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ab',\n",
       " 'across',\n",
       " 'adher',\n",
       " 'among',\n",
       " 'andor',\n",
       " 'anticip',\n",
       " 'antigen',\n",
       " 'arm',\n",
       " 'attack',\n",
       " 'background',\n",
       " 'base',\n",
       " 'case',\n",
       " 'chang',\n",
       " 'circul',\n",
       " 'clinic',\n",
       " 'clinicaltrialsgov',\n",
       " 'cluster',\n",
       " 'collect',\n",
       " 'complet',\n",
       " 'compos',\n",
       " 'conclusionssignific',\n",
       " 'conduct',\n",
       " 'confirm',\n",
       " 'contact',\n",
       " 'control',\n",
       " 'correspond',\n",
       " 'countri',\n",
       " 'cultur',\n",
       " 'data',\n",
       " 'day',\n",
       " 'definit',\n",
       " 'depend',\n",
       " 'design',\n",
       " 'diagnos',\n",
       " 'differ',\n",
       " 'drift',\n",
       " 'due',\n",
       " 'durat',\n",
       " 'efficaci',\n",
       " 'feasibl',\n",
       " 'find',\n",
       " 'follow',\n",
       " 'hand',\n",
       " 'hkclinicaltrialscom',\n",
       " 'hkctr',\n",
       " 'home',\n",
       " 'hong',\n",
       " 'hour',\n",
       " 'household',\n",
       " 'hygien',\n",
       " 'ill',\n",
       " 'implement',\n",
       " 'index',\n",
       " 'influenza',\n",
       " 'inform',\n",
       " 'intervent',\n",
       " 'issu',\n",
       " 'kong',\n",
       " 'laboratori',\n",
       " 'lack',\n",
       " 'later',\n",
       " 'learnt',\n",
       " 'least',\n",
       " 'lesson',\n",
       " 'like',\n",
       " 'lower',\n",
       " 'main',\n",
       " 'mask',\n",
       " 'measur',\n",
       " 'member',\n",
       " 'methodologyprincip',\n",
       " 'nct',\n",
       " 'nose',\n",
       " 'outcom',\n",
       " 'pattern',\n",
       " 'perhap',\n",
       " 'pharmaceut',\n",
       " 'pilot',\n",
       " 'present',\n",
       " 'primari',\n",
       " 'quickvu',\n",
       " 'random',\n",
       " 'rapid',\n",
       " 'ratio',\n",
       " 'recent',\n",
       " 'recruit',\n",
       " 'reduc',\n",
       " 'registr',\n",
       " 'relat',\n",
       " 'report',\n",
       " 'secondari',\n",
       " 'self',\n",
       " 'signific',\n",
       " 'significantli',\n",
       " 'spars',\n",
       " 'spread',\n",
       " 'strain',\n",
       " 'studi',\n",
       " 'subject',\n",
       " 'surgic',\n",
       " 'suscept',\n",
       " 'swab',\n",
       " 'symptom',\n",
       " 'symptomat',\n",
       " 'test',\n",
       " 'throat',\n",
       " 'transmiss',\n",
       " 'trial',\n",
       " 'vari',\n",
       " 'variabl',\n",
       " 'viral',\n",
       " 'viru',\n",
       " 'visit',\n",
       " 'whether',\n",
       " 'within'}"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "sir.get_similarity(query[n],corpus[11]['text'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "print(query[n],'\\n\\n',corpus[11]['text'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "coronavirus origin \n",
      "\n",
      " BACKGROUND: Pediatric LRTI hospitalizations are a significant burden on patients, families, and healthcare systems. This study determined the burden of pediatric LRTIs on hospital settings in British Columbia and the benefits of prevention strategies as they relate to healthcare resource demand. METHODS: LRTI inpatient episodes for patients <19 years of age during 2008–2010 were extracted from the BC Discharge Abstract Database. The annual number of acute care beds required to treat pediatric LRTIs was estimated. Sub-analyses determined the burden due to infants <1 year of age and high-risk infants. Population projections were used to forecast LRTI hospitalizations and the effectiveness of public health initiatives to reduce the incidence of LRTIs to 2020 and 2030. RESULTS: During 2008–2010, LRTI as the primary diagnosis accounted for 32.0 and 75.9% hospitalizations for diseases of the respiratory system in children <19 years of age and infants <1 year of age, respectively. Infants <1 year of age accounted for 47 and 77% hospitalizations due to pediatric LRTIs and pediatric LRTI hospitalizations specifically due to respiratory syncytial virus (RSV), respectively. The average length of stay was 3.1 days for otherwise healthy infants <1 year of age and 9.1 days for high-risk infants (P <0.0001). 73.1% pediatric LRTI hospitalizations occurred between November and April. Over the study timeframe, 19.6 acute care beds were required on average to care for pediatric LRTIs which increased to 64.0 beds at the peak of LRTI hospitalizations. Increases in LRTI bed-days of 5.5 and 16.2% among <19 year olds by 2020 and 2030, respectively, were predicted. Implementation of appropriate prevention strategies could cause 307 and 338 less LRTI hospitalizations in <19 year olds in 2020 and 2030, respectively. CONCLUSION: Pediatric LRTI hospitalizations require significant use of acute care infrastructure particularly between November and April. Population projections show the burden may increase in the next 20 years, but implementation of effective public health prevention strategies may contribute to reducing the acute care demand and to supporting efforts for overall pediatric healthcare sustainability.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "qj = 0.1\r\n",
    "sj = 0.4\r\n",
    "ss = 0.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "(qj*(1-ss))+(sj*ss)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "sj*(ss)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "(qj+sj)/(1/ss)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6000000000000001"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "a = {1,2,3}\r\n",
    "b = {2,3,4}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "len(b.intersection(a))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "ex = corpus[0]['text']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get set intersection. Discount factor determines the divisor for the jaccard score of the either synonyms, or clustered words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "ex = preprocess(ex)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'chang', 'coronaviru', 'respons', 'weather'}"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b1f09a33527022494cbc1a3254192479adc88b8f7fdce239975774d86eaf2091"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}